{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c865f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /home/doaa/miniconda3/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/doaa/miniconda3/lib/python3.13/site-packages (from nltk) (1.5.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /home/doaa/miniconda3/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14f8b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/doaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ee0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_stopwords = set([\n",
    "    \"في\", \"من\", \"على\", \"و\", \"ما\", \"عن\", \"إلى\", \"أن\", \"إن\", \"لا\", \"لم\", \"لن\", \"هو\", \"هي\", \"هذا\", \"هذه\", \n",
    "    \"ذلك\", \"تلك\", \"هناك\", \"هنا\", \"ثم\", \"قد\", \"حتى\", \"إذا\", \"بين\", \"كان\", \"كانت\", \"يكون\", \"نكون\", \"مع\", \"كل\",\n",
    "    \"كما\", \"لكن\", \"أو\", \"أي\", \"أين\", \"كيف\", \"هل\", \"أجل\", \"بل\", \"ف\", \"ب\", \"أ\", \"ل\", \"إ\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bdf4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"و\", text)\n",
    "    text = re.sub(\"ئ\", \"ي\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"ـ\", \"\", text)\n",
    "    text = re.sub(r'[ًٌٍَُِّْ]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88225024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # حذف الرموز\n",
    "    text = re.sub(r'\\d+', '', text)       # حذف الأرقام\n",
    "    text = re.sub(r'\\s+', ' ', text)      # توحيد المسافات\n",
    "    text = normalize_arabic(text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f78aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/home/doaa/programming/NLP/arwiki_plaintext.txt\"\n",
    "processed_sentences = []\n",
    "max_lines = 20000\n",
    "subset = []\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= max_lines:\n",
    "            break\n",
    "        cleaned = clean_text(line)\n",
    "        tokens = word_tokenize(cleaned)\n",
    "        tokens = [t for t in tokens if t not in arabic_stopwords and len(t) > 2]\n",
    "        processed_sentences.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05468210",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"subset_arwiki.txt\", \"w\", encoding=\"utf-8\") as out:\n",
    "    for line in subset:\n",
    "        out.write(line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd9d10ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['ماء']\n",
      "['يسار', 'تصغير', 'xبك', 'الماء', 'المركب', 'الكيميايي', 'الاكثر', 'وفره', 'الارض']\n",
      "['يسار', 'تصغير', 'xبك', 'الماء', 'حالاته', 'الثلاث', 'السايله', 'والصلبه', 'جليد', 'والغازيه', 'بخار', 'ماء', 'سحاب']\n",
      "['الماء', 'ماده', 'شفافه', 'عديمه', 'اللون', 'والرايحه', 'وهو', 'المكو', 'الاساسي', 'للجداول', 'والبحيرات', 'والبحار', 'والمحيطات', 'وكذلك', 'للسوايل', 'جميع', 'الكاينات', 'الحي', 'وهو', 'اكثر', 'المرك', 'بات', 'الكيميايي', 'انتشارا', 'علي', 'سطح', 'الارض', 'يتال', 'جزيء', 'الماء', 'اكسجين', 'مركزيه', 'ترتبط', 'بها', 'هيدروجين', 'علي', 'طرفيها', 'برابطه', 'تساهمي', 'بحيث', 'تكون', 'صيغته', 'الكيمياييه', 'عند', 'الظروف', 'القياسيه', 'الضغط', 'ودرجه', 'الحراره', 'الماء', 'سايلا', 'الحاله', 'الصلبه', 'فتتشك', 'عند', 'نقطه', 'التجم', 'وتدعي', 'بالجليد', 'الحاله', 'الغازيه', 'فتتشك', 'عند', 'نقطه', 'الغليان', 'وتسم', 'بخار', 'الماء']\n"
     ]
    }
   ],
   "source": [
    "for sent in processed_sentences[:5]:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51b4001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "stemmer = ISRIStemmer()\n",
    "input_path = \"/home/doaa/programming/NLP/arwiki_plaintext.txt\"\n",
    "output_path = \"/home/doaa/programming/NLP/preprocessed_arwiki.txt\"\n",
    "\n",
    "processed_sentences = []\n",
    "max_lines = 20000\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= max_lines:\n",
    "            break\n",
    "        cleaned = clean_text(line)\n",
    "        tokens = word_tokenize(cleaned)\n",
    "        filtered_tokens = [t for t in tokens if t not in arabic_stopwords and len(t) > 2]\n",
    "        stemmed_tokens = [stemmer.stem(t) for t in filtered_tokens]\n",
    "        processed_sentences.append(stemmed_tokens)\n",
    "\n",
    "# ✅ حفظ النتيجة في ملف\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for tokens in processed_sentences:\n",
    "        f.write(\" \".join(tokens) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f20327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed sentences saved to /home/doaa/programming/NLP/preprocessed_arwiki.txt\n"
     ]
    }
   ],
   "source": [
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for tokens in processed_sentences:\n",
    "        f.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "print(f\"Processed sentences saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27b084a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/doaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['ماء']\n",
      "['يسار', 'تصغير', 'xبك', 'الماء', 'المركب', 'الكيميايي', 'الاكثر', 'وفره', 'الارض']\n",
      "['يسار', 'تصغير', 'xبك', 'الماء', 'حالاته', 'الثلاث', 'السايله', 'والصلبه', 'جليد', 'والغازيه', 'بخار', 'ماء', 'سحاب']\n",
      "['الماء', 'ماده', 'شفافه', 'عديمه', 'اللون', 'والرايحه', 'وهو', 'المكو', 'الاساسي', 'للجداول', 'والبحيرات', 'والبحار', 'والمحيطات', 'وكذلك', 'للسوايل', 'جميع', 'الكاينات', 'الحي', 'وهو', 'اكثر', 'المرك', 'بات', 'الكيميايي', 'انتشارا', 'علي', 'سطح', 'الارض', 'يتال', 'جزيء', 'الماء', 'اكسجين', 'مركزيه', 'ترتبط', 'بها', 'هيدروجين', 'علي', 'طرفيها', 'برابطه', 'تساهمي', 'بحيث', 'تكون', 'صيغته', 'الكيمياييه', 'عند', 'الظروف', 'القياسيه', 'الضغط', 'ودرجه', 'الحراره', 'يكون', 'الماء', 'سايلا', 'الحاله', 'الصلبه', 'فتتشك', 'عند', 'نقطه', 'التجم', 'وتدعي', 'بالجليد', 'الحاله', 'الغازيه', 'فتتشك', 'عند', 'نقطه', 'الغليان', 'وتسم', 'بخار', 'الماء']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "arabic_stopwords = set([\n",
    "    \"في\", \"من\", \"على\", \"و\", \"ما\", \"عن\", \"إلى\", \"أن\", \"إن\", \"لا\", \"لم\", \"لن\", \"هو\", \"هي\", \"هذا\", \"هذه\", \n",
    "    \"ذلك\", \"تلك\", \"هناك\", \"هنا\", \"ثم\", \"قد\", \"حتى\", \"إذا\", \"بين\", \"كان\", \"كانت\", \"يكون\", \"نكون\", \"مع\", \"كل\",\n",
    "    \"كما\", \"لكن\", \"أو\", \"أي\", \"أين\", \"كيف\", \"هل\", \"أجل\", \"بل\", \"ف\", \"ب\", \"أ\", \"ل\", \"إ\"\n",
    "])\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"و\", text)\n",
    "    text = re.sub(\"ئ\", \"ي\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"ـ\", \"\", text)\n",
    "    text = re.sub(r'[ًٌٍَُِّْ]', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = normalize_arabic(text)\n",
    "    return text.strip()\n",
    "\n",
    "input_path = \"/home/doaa/programming/NLP/arwiki_plaintext.txt\"\n",
    "output_path = \"/home/doaa/programming/NLP/preprocessed_arwiki_for_autocomplete.txt\"\n",
    "\n",
    "processed_sentences = []\n",
    "max_lines = 100000\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= max_lines:\n",
    "            break\n",
    "        cleaned = clean_text(line)\n",
    "        tokens = word_tokenize(cleaned)\n",
    "        filtered_tokens = [t for t in tokens if len(t) > 2]\n",
    "        processed_sentences.append(filtered_tokens)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for tokens in processed_sentences:\n",
    "        f.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "for sent in processed_sentences[:5]:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c1a7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
